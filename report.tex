\documentclass[twoside,12pt]{article}
\usepackage{moreverb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{appendix}
\usepackage[margin=1in]{geometry}
\usepackage{framed}
\usepackage{enumerate}
\usepackage{hyperref}
\hypersetup{
 colorlinks=true,
 urlcolor=blue
}

\pagenumbering{gobble}

\newcommand{\class}{NLP}
\newcommand{\name}{Final Project Report}
\newcommand{\due}{December 10, 2015}
\newcommand{\code}[1]{\texttt{#1}}

\newenvironment{solution}
{\begin{framed} \textbf{Solution:}\par}
{\end{framed}}

\newcommand{\pic}[3][width=\textwidth]
{
  \begin{figure}
  \centering
  \includegraphics[#1]{#2}
  \caption{#3}
  \label{#2}
  \end{figure}
}

\begin{document}
\date{\due}
\title{\class{} \name }
\author{ Tobin Yehle and Dasha Pruss}
\maketitle


\section{Components and Contributions}

All of the NLP code is in \code{Sherlock.java}. We have a \code{main} method inside of \code{Driver.java}, which does all the IO and data wrangling. We made some data structures for storing stories, which are in \code{Story.java}. Most of this code was auto-generated (\code{equals}, \code{toString}, etc.).

Our \code{Sherlock.java} file contains a single public function, \code{processStory}, which calls the numerous private methods that we wrote to first find the best sentence in the document, and then extract key phrases from that sentence.

We pair programmed our entire project and each of us spent roughly equal time programming. The only file of real substance was \code{Sherlock.java}, and it underwent many changes. If you would like to see the specifics of who edited what, feel free to browse the commit history of our project in our
\href{https://bitbucket.org/tobinyehle/sherlock/commits/all}{bit bucket repository}. Generally the name on the commit was the person typing, while the other team member dished out advice.
To see who last edited each line of a file you can take a look at
\href{https://bitbucket.org/tobinyehle/sherlock/annotate/e57a8467bc90e614b13ca2c99f0b5d681baa65c9/src/cs/utah/sherlock/Sherlock.java?at=default&fileviewer=file-view-default}{blame}.

\section{External Resources}

We used Stanford's CoreNLP libraries for tokenization, sentence splitting, part-of-speech tagging, NER, parsing, and coreference resolution. We used their models that were trained with news article corpora, such as CoNLL.

\section{Regrets and Successes}

We regret spending so much of our time trying to get co-reference resolution to work because it ended up not helping us at all --- in fact, it made our f-score worse. Instead of spending our time on coreference, we wish we had used WordNet to find semantic classes our of noun phrases to improve our phrase extraction procedure.

The rules that we implemented, both from the Quarc paper as well as our own rules, were highly effective at identifying the correct sentence. We were also happy with the way we extracted phrases from the sentences --- it was a very simple approach, but surprisingly effective.

\end{document}
